\chapter{Design of Adaptivity}
\label{chap:design-of-adaptivity}

In the last chapter, we have mentioned how we incorporated many of the
strategies to support learning and motivation into the game.
In this chapter, we discuss another strategy which helps to achieve the state of
flow, the one that is special to ALS  % TODO:reformulate
-- recommending tasks of optimal difficulty.
% ... "which is the secret sauce of adaptive learning systems"
% ... "which is powered by the AI"
% TODO: More links to learning/motivation \label{sec:motivation.challenge}
% TODO: link to the adaptivity chapter

\section{Expected Behavior}  % or "Specification", "Goals", "System Behavior", "Behavior specification"
\label{sec:robomission.behavior}

% NOTE: This section should provide a useful basis for:
% - guide how to design ALS components
% - setting rewards for a a RL/planning agent
% - defining "good recommendations" (~good performance) for supervised
%   learning and for use in monitoring/evaluation (domain metric)

% TODO: clarify the relationship between this section and AL.analysis (or
% possibly also this-chapter.analysis) sections.

Wide range of task difficulties, combined with the adaptive behavior,
should make the system useful for anybody who wants to learn
introductory programming.
However, this long-term goal requires many iterations and a lot of data.
Currently, the system primarily targets at children 10-15 years old.
% "between 10 and 15 years."

The main use case of the system is a 1-2 hours in-classroom tutorial
(``Hour of Code'' style), with a possible follow-up individual practice at home.
In line with the system mission (\cref{sec:mission}), goals of this tutorial
are to teach the student basic concepts of programming,
help them to spend most of the time in the state of flow,
and motivate them for the further learning of computer science.
% TODO: better link to fullfilling human needs (as stated in table in chapter 2)
% TODO: better terminoglogy, because "You cannot motivate people... (motivation
% goes fron inside)"
% NOTE: flow is a goal; but at the same time it supports the other 2 goals

After each solved task, the student is shown a dialog window with one
recommended task, and with a link to the page with overview of all tasks.
% Terminology: \emph{good recommendation}
Ideally, each recommendation should lead to the state of flow.
% TODO: How to measure? One way: looking ath the long-term objectives
As we cannot directly observe whether the student is in the state of flow,
we must rely on proxy data, which can be either subjective
(using perceived difficulty ranking),
or objective (using observed performance), %should be neither too low, nor too high),
% "qualitative" vs. "quantitative"
% TODO: REF:performance
as we have discussed in section
on attributable metrics (\cref{sec:live-evaluation}).
% long-term or attributable metrics \cref{sec:long-term-objectives,sec:live-evaluation}
% TODO: the performance compression should be described in this chapter
% in student modeling section, ideally with an analysis supporting the
% decsions or sketching some possible directions.

% TODO: Reformulate: (e.g. why is it useful, useful for what)
Unfortunately, both subjective and objective approach to measure flow
is very noisy. Therefore, it is useful to formulate weaker, but better observable
requirements on the system behavior:
% NOTE: It reminds me of hypothesis testing of goodness-of-fit: there are
% various tests (metrics we can measure), that can tell us, that the given
% sample is definitely not from the compared distribution; some of these
% tests are very simple and intuitive (e.g. for exp. distribution, ratio
% of mean and stdev^2 should be 1), but they also have quite small power
% (i.e. they are often not able to reject false hypotheses).
% TODO: shrink vertical spaces
\begin{itemize}
\item The student is able to solve any task recommended by the system, ideally
  in a reasonable time (20 minutes).
    Furthermore, the student is able to solve the first few tasks quickly
    (in 2 minutes) and progress to the second level quickly (in at most 10
    minutes).
\item The best performing students (supposedly with a prior programming skill)
should progress through a first few levels quickly, spending at most 5 minutes
on tasks practicing only sequences of commands, and get to the more
challenging tasks containing both types of loops and conditional statements
in at most 20 minutes.  % 10, 15?
\item All students should gradually start practicing all basic programming concepts
  (sequence, loops, conditional statements) during the first hour of the tutorial.
  % TODO: check gvid
\item Average difficulty of the recommended tasks should gradually increase.
  (Occasionally, an easy tasks might be recommended in order to improve
  exploration, but these easy tasks should
  not be too frequent, and they should be differentiated for the student,
  e.g. as \emph{speed challenges} in order to explain the sudden change in
  difficulty.)
\item At most one new programming concept and one new game concept should appear
  in a task. No new concept should be introduced, until the student
  solves at least one task with the last introduced concept with a good
  performance. % (they must sometimes come together, it is not a problem)
% TODO: perform analysis if it's the case in the current system.
%Each new concept (e.g. block) is explained in the task where it appears for the first time. (REF: img), so the student understands all elements in the game world and blocks available in the toolbox.
\item If asked after each task, students should report that the task was
  neither too easy, nor too difficult for them in at least half of the tasks
  during the tutorial.  % or another negative tag, such as boring, weird, ...
\item Neither task that was reported as too easy should take them
  more than 1 minute, and there should never be more then 3 too easy tasks
  in a row (with the exception of the first level).
  %(Student is not bored by neither too easy task requiring many commands (i.e.
  %taking more than 1 minute to build the streightforward solution), nor by the
  %sequence of either too easy or too similar tasks.
  % TODO: Maybe it would be better to state it in combined max too-easy time?
  % NOTE: It is ok to have sometimes an easy task (esp. at the beginning), but
  % they should be quick to solve and not too frequent
\item If asked at the beginning and at the end of the tutorial about their
  interest in learning computer science, the end report should be statistically
  higher.
% TODO: Not sure if to mention the self-report points, as they are not
% currently used in the system.
% TODO: (Another flow-related) If the student marks the current task as too
% easy, the next task is strictly more difficult. If the student marks the
% current task as too difficult, the next task is as most as difficult as the
% current one.
\end{itemize}
% TODO: compare these requiremnts againts collected data and gvid-report



%\subsection{Main Use Case}
%\label{sec:robomission.use-case}
%
%\begin{itemize}
%\item Student visits the home page of the project, reads the "promotional slides" and tries the game with manual controls. On the last slide, they click on the recommended task from the 1st level.
%(REF: img)
%\item Student creates the program using Blockly blocks and can run the program as many times as needed. (REF: img) Program execution is visualized. Student can change the speed of the execution.
%\item After each unsuccessful execution, a short message explaining why the task was not solved is shown (e.g. "The spaceship must reach the final row.") (REF: img)
%\item Student is able to solve the first few tasks quickly (within 2 minutes).
%\item After solving each task, student is shown a visualization of obtaining points (called credits) (REF: printscreen). After a few solved tasks, student progresses to next level.
%\item After each solved task, student is shown a dialog with one recommended task, and also a link to the page with overview of all tasks (REF: img).
%\item Student is able to solve any task recommended by the system (within 15 minutes).
%\item Each new concept (e.g. block) is explained in the task where it appears for the first time. (REF: img), so the student understands all elements in the game world and blocks available in the toolbox.
%\item Students with some prior programming skill should progress through first few levels quickly (within 10 minutes) and get to the more challenging tasks containing both types of loops, conditionals etc.
%\item Student is not bored by neither too easy task requiring many commands (i.e. taking more than 1 minute to build the streightforward solution), nor by the sequence of either too easy or too similar tasks.
%\item Student can sign up (or log in without registration through their social accounts) at any moment to save their progress. Even without singing up, the system can associate the student with its progress using session cookie (but also provides a button to clear the history).
%\item Student can provide a feedback or report a bug easily (and the feedback is send to admins by an email).
%\end{itemize}
%(TBA: add diagram with images for all these steps linked by arrows showing transitions)

% TODO: consider some of the following notes
% - Design of tasks for the system is described in section \ref{sec:robomission.tasks}.
% - Adaptive aspect of the behavior is described in section \ref{sec:robomission.adaptability}.
%\item intuitive and simple user interface crucial (aiming at children, they need to focus on learning programming, it would be bad to waste their mental power on understanding a complex interface)
%\item mini-instructions (ref to the Google research on ignoring instructions, show how it was solved in Blockly Games; ref figure)
%\item mini-explations (difference from instructions: after the fact) (ref figure) (they also serve as a convenient mean to game resetting)
%\item motivation: intrinsic (fun challenging game + optimal difficulty) and simple external motivation scheme: credits and levels


%\subsection{Four Modes of Usage}
%\label{sec:robomission.use-cases}
%
%In addition to the main use case described in the previous section,
%which assumes a new student without any context (e.g. a classroom),
%the system can be potentially used in other (or more specific) ways.
%
%\begin{itemize}
%\item "Hour of Code" mode
%  \begin{itemize}
%  \item single hour
%  \item mainly as a motivation to programming
%  \item using RoboBlocks
%  \item directly at elemenatry and high schools, or at home
%  \item plus: MjUNI workshop
%  \item shorter promotianal version for DODs (?) ("10 minutes of code")
%  \item (should be strictly time-limited; certificate at the end)
%  \end{itemize}
%\item "Foundations mode" individual learning of elementary programming (individual at home or in a classroom, from several days to several weeks, depending on the prior skill); natural continuation of the first "Hour of Code" (next levels, with RoboBlocks)
%\item "University mode" -- levels with RoboCode/Python, at home / secondary schools, KSI (0th problem set), IB111 (0th/1st motivational lesson - needs Python and to be better than turtle)
%\item "Competition mode" -- competitions such as Purkiada, Pevnost FI, KSI
%(advanced problem sets), new FIBot (physical version already in InterSoB 2017,
%then in Sob 2018); this also includes testing mode for RH interns
%\end{itemize}
%
%All these modes can be naturally implemented as distinct levels,
%going from the easy tasks using RoboBlocks for "Hour of Code",
%gradually transitioning to the RoboCode during learning the "Foundations",
%using full-fledged Python for "University mode"
%and offering both blocks and Python for the individual competitions.
%Levels from the past competitions can be made public for all students.


\section{Domain Model}

%In this phase of development, we have decided not to model overlapping concepts,
% NOTE: not enough data, not clear how they combine and how they relate to PS
% which are still needed for the users
Currently, we do not model overlapping concepts, since their use would require
more data to analyze their interaction.
Instead, we devide tasks into disjoint linearly order hierarchical problem
sets (\cref{fig:robomission.domain}). % , which are easier to handle.
% ... We started by manually dividing tasks into 9 problem sets (levels)
The hierarchy has two levels: the top-level problem sets are call \emph{missions},
and they contain about 8-10 tasks, which are further split into three smaller
problem sets called \emph{phases}.
Missions and phases are ordered, while tasks within a given phase are not.
The missions were chosen primarily by the available programming blocks and
included game elements, and secondarily by an estimated difficulty.
% (initialy human-estimated, later refined using collected data
% NOTE: Data collected befor division into phases (TODO: mention in the
% analysis chapter...)
The contribution of refining the missions into phases is threefold:
\begin{itemize}
\item Phases enforce important prerequisities within a mission (e.g.
introducing wormholes before using them in more difficult tasks).
\item Phases help to achieve a balanced composition \cite{progression-analysis},
  introducing new concepts in the first phase,
  recombining them with previously learned concepts in the second phase,
  and further reinforcing (practicing) of known concepts in the third phase.
% TODO: Although this division is only a guide, doesn't hold exatly...
% TODO: elaborate on the relevance of the paper
% TODO: check the paper (terms of the phases, their "oreder" and meaning)
%The resulting two-level hierarchical and ordered problem set structure is shown
%in \cref{fig:robomission.domain}.
\item Phases are approximately homogeneous, i.e. difficulty of all tasks in a
  given phase is similar. This allows allows for simpler tutor models.
  % and they practic similar concepts
\end{itemize}
% TODO: link to what presented in earlier chapters

\begin{figure}[htb]
\centering
TODO: domain diagram\\
(all 9 missions with labels; unlabeled nodes for phases and tasks)
% should show covered concepts: sequences of commands, repeat, while, if, if-else, simple tests (comparing)
\caption{Domain model used in RoboMission.}
\label{fig:robomission.domain}
\end{figure}


\section{Student Model}

\begin{itemize}
\item skill for each chunk (PS) (or rather "verified skill"), between 0 and 1
\item update: phase skill $s^0_f \leftarrow 0$;
  $s^{t+1}_f \leftarrow \min(1, s^t_f + \max(p, \frac{1}{|F_f|}))$
  (TODO: explain + notation: f, p, F)
\item update: mission skill: just average of phase skills (huge simplification...)
\item TODO: link to what presented in earlier chapters
\item TODO: discretized performance (elaborate: values, how computed, REF)
\end{itemize}


\section{Tutor Model}

\begin{itemize}
\item PS selection: first unmastered
  (first unmastered phase of first numastered mission)
  (reasoning: students want "green everything", small number of PS)
\item task selection: random + filter to avoid solved tasks
  (reasoning: interchangeable tasks; exploration maximization)
\item mastery decision: student model above + threshold exactly 1 (?)
\item TODO: link to what presented in earlier chapters
\end{itemize}




TODO: flow wrt to missions and phases:
  progressing to a new mission or phase increase the challenge,
  then the perceived challenge drops during a single phase as student is getting
  more skill; result in increasing "wave" flow; recommended by \cite{book-of-lenses}
  (\cref{fig:robomission.flow})

\begin{figure}[htb]
\centering
TODO: flow diagram specifically for progress through missions and phases
(increase of challenge with each new phase, then decrease during the phase)
\caption{Dramaturgy of flow in RoboMission.}
\label{fig:robomission.flow}
\end{figure}






% TODO? UI? Probably not enough material for this



\section{Analysis Layer}
\label{sec:robomission.analysis-layer}

Even with careful testing, there will always be some errors in the deployed systems.
For example, there might be a task that is much easier than expected,
due to missing limit or some other mistake in its setting.
In addition to the errors in data,  % TODO: fix parallelism
the adpative behavior of the system is extremelly difficult to test as well.
As discused in section \ref{sec:metrics-and-evaluation} is only a proxy for our true objectives
and can be sometimes misleading.
As a result, it is not even more practical, but even neccessary to deploy system
that is imperfect, and then find and fix the problems that arise.
To spot the problems as soon as possible,
monitoring is a crucial component,
that facilites iterative improvement of the system.
% TODO: ref to iterative improvement section / rule of the loop principle

\subsection{Admin Requirements}
\label{sec:admin-requirements}

Similarly to regular users, administrators also have requirements on the systems:

\begin{itemize}
\item Admin can immediately see how much is the system used and how the system behaves with respect to the short-term ("live-evaluation") metrics (...)
\item Admin receives feedback from provided by users, and error reports on unhandled exceptions.
\item Admin can see metrics on individual tasks (to quickly detect issues with a task).
\end{itemize}


\subsection{Monitoring Components}

To fulfill the requiremetns from section \ref{sec:admin-requirements},
the system includes the following components:

\begin{itemize}
\item \textbf{Google Analytics} --
  shows distribution of users with respect to time and space.
  In addition to page visits,
  it can also process specific events sent from the frontend,
  such as clicking on the execution button,
  and divide them into groups, e.g. by task or group for AB experiment.
  (figure \ref{fig:google-analytics})
\item \textbf{Monitoring Dashboard} (see figure \ref{fig:monitoring-dashboard})
      which shows values of metrics for last month.
      (Computed metrics are described in section \ref{sec:robomission.metrics}.)
\item \textbf{User Feedback} --
  a modal form that can be opened by a user on any page
  to send us a message that something is not working as expected.
  Each user feedback is send to the administrators by an email.
\item \textbf{Error Reports} --
  if an unhandled top-level error occur on the server,
  it is not only logged, but also send to the administrators.
\item \textbf{Data Exports} --
  All collected data are every week exported as a zip bundle containing
  CSV files prepared for convenient offline analysis.
\item \textbf{Investigaion Notebook} --
  a template of jupyter notebook with a command that generates exports
  and serves them directly as pandas data frames
  (see figure \ref{fig:investigation-notebook}).
\item \textbf{Logs} --
  all requests, performed actions, unhandled errors and submitted feedback are logged
  to files on the server for manual inspection.
\end{itemize}


\imgW[0.7]{google-analytics}{Preview from Google Analytics (breakdown for "execution" event).}

% TODO: latest dashboard, ideally with readable values
\imgW[0.6]{monitoring-dashboard}{Metrics visualized in the monitoring dashobard.}

% TODO: update investigation notebook (fix error in last cell, current data;
% should show something interesting, or at least some descriptive analylis)
\imgW[0.7]{investigation-notebook}{Template of jupyter notebook for investigation of live data.}


\subsection{Metrics}
\label{sec:robomission.metrics}

% TODO: include live metrics if there are any
Every night, the system recomputes metrics related to the long-term objectives
(as described in \ref{sec:long-term-objectives}):

\begin{itemize}
\item "daily active students" = number of users who solved at least 1 task this day,
\item "solving hours" = total time spent on successful attempts,
\item "success ratio" = proportion of successful attempts,
\item number of solved tasks.
%\item (TBA: update if it changed)
\end{itemize}


The system also computes metrics about each task (in order to e.g. detect problematic ones):
\begin{itemize}
\item solved count,
\item median time,
\item success ratio.
\end{itemize}




% TODO: consider if to include non-functional requirements or not
%\section{Non-functional Requirements}
%\label{sec:robomission.nonfunctional-requirements}
%
%\begin{itemize}
%\item easy to understand code, pleasure to read and write (extend)
%\item easy to refactor and add new things (new tasks, levels, recommendation strategies etc.)
%\item robust, efficient, interpretable behavior
%\end{itemize}


\section{Rule of the Loop}
\label{sec:robomission.rule-of-the-loop}

\begin{itemize}
\item the design interleaved with the implementation -> final design is completely different than the original one
\item ref: rule of the loop
  (CITE: in game design \cite{book-of-lenses},
  in machine learning \cite[][Rule \#16]{google-ml-rules},
  for WS development: SCRUM; our project includes all these 3 elements)
\item first prototype: 2016, one year of development, thrown away; for testing our initial ideas and find what works and what not; 50 tasks with a robot in maze
  \begin{itemize}
  \item problems with the robot in the maze: not fun (boring, not inovative, repetitiveness), did not allow for a plenty of diverse easy tasks (which is necessary for adaptive systems), required a lot of blocks even for simple programs (compared to the SpaceGame)
  \item problems with the codebase (maintainability, extensibility - why?)
  \item and the good things? (SPA, explored/verified useful technologies, such as Blockly and Django)
  \end{itemize}
\end{itemize}

\imgW{prototype-task-environment}{First prototype of the system, with a classic robot-in-maze game.}
